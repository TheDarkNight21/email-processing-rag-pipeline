{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email RAG Pipeline\n",
    "\n",
    "This notebook demonstrates a simple RAG (Retrieval-Augmented Generation) pipeline for processing downloaded Outlook emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:20:19.172309Z",
     "start_time": "2025-09-24T15:20:18.651795Z"
    }
   },
   "source": [
    "import os\n",
    "import email\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "\n",
    "# For vector operations (you'll need to install these)\n",
    "import numpy as np\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "#import faiss\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Email Files from Local Directory\n",
    "\n",
    "Load your downloaded Outlook email files (.eml, .msg, or .pst files) from a local directory\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:21:46.112921Z",
     "start_time": "2025-09-24T15:21:46.105450Z"
    }
   },
   "source": [
    "# Set the path to your email directory\n",
    "email_directory = \"data/raw_emails\"  # Change this to your email folder path\n",
    "\n",
    "# Load email files from local directory\n",
    "email_paths = {}\n",
    "if os.path.exists(email_directory):\n",
    "    # Find all email files in the directory\n",
    "    email_files = []\n",
    "    for ext in ['*.eml', '*.msg', '*.pst']:\n",
    "        email_files.extend(Path(email_directory).glob(ext))\n",
    "    \n",
    "    # Create a mapping of email files\n",
    "    for i, file_path in enumerate(email_files):\n",
    "        email_paths[f\"Email-{i+1}\"] = str(file_path)\n",
    "    \n",
    "    print(f\"Found {len(email_paths)} email files in {email_directory}:\")\n",
    "    for name, path in email_paths.items():\n",
    "        print(f\"  {name}: {os.path.basename(path)}\")\n",
    "else:\n",
    "    print(f\"Directory {email_directory} not found!\")\n",
    "    print(\"Please create the directory and add your email files, or update the 'email_directory' variable above.\")\n",
    "    print(\"Example: email_directory = '/path/to/your/email/folder'\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 email files in data/raw_emails:\n",
      "  Email-1: First Merchants Bank Royal Oak_ Invitation to bid on First Merchants Bank.eml\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Email Parsing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:02.862244Z",
     "start_time": "2025-09-24T15:22:02.856064Z"
    }
   },
   "source": [
    "def parse_eml_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse .eml email file and extract text content\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        msg = email.message_from_bytes(f.read())\n",
    "    \n",
    "    # Extract email metadata\n",
    "    email_data = {\n",
    "        'subject': msg.get('Subject', ''),\n",
    "        'from': msg.get('From', ''),\n",
    "        'to': msg.get('To', ''),\n",
    "        'date': msg.get('Date', ''),\n",
    "        'body': ''\n",
    "    }\n",
    "    \n",
    "    # Extract email body\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            if content_type == \"text/plain\":\n",
    "                email_data['body'] = part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "                break\n",
    "            elif content_type == \"text/html\" and not email_data['body']:\n",
    "                # Fallback to HTML if no plain text\n",
    "                email_data['body'] = part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "    else:\n",
    "        email_data['body'] = msg.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "    \n",
    "    return email_data\n",
    "\n",
    "def parse_msg_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse .msg email file (requires python-msg-parser library)\"\"\"\n",
    "    try:\n",
    "        from msg_parser import MsOxMessage\n",
    "        \n",
    "        msg = MsOxMessage(file_path)\n",
    "        \n",
    "        email_data = {\n",
    "            'subject': msg.subject or '',\n",
    "            'from': msg.sender or '',\n",
    "            'to': msg.to or '',\n",
    "            'date': str(msg.date) if msg.date else '',\n",
    "            'body': msg.body or ''\n",
    "        }\n",
    "        \n",
    "        return email_data\n",
    "    except ImportError:\n",
    "        print(\"python-msg-parser not installed. Install with: pip install python-msg-parser\")\n",
    "        return None\n",
    "\n",
    "def parse_email_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse email file based on extension\"\"\"\n",
    "    file_ext = Path(file_path).suffix.lower()\n",
    "    \n",
    "    if file_ext == '.eml':\n",
    "        return parse_eml_file(file_path)\n",
    "    elif file_ext == '.msg':\n",
    "        return parse_msg_file(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_ext}\")\n",
    "        return None\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Text from Uploaded Emails\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:05.654216Z",
     "start_time": "2025-09-24T15:22:05.642131Z"
    }
   },
   "source": [
    "# Extract text from uploaded emails\n",
    "email_texts = {}\n",
    "email_metadata = {}\n",
    "\n",
    "for email_name, filename in email_paths.items():\n",
    "    print(f\"Processing {filename}...\")\n",
    "    \n",
    "    # Parse the email file\n",
    "    email_data = parse_email_file(filename)\n",
    "    \n",
    "    if email_data:\n",
    "        # Store metadata\n",
    "        email_metadata[email_name] = {\n",
    "            'subject': email_data['subject'],\n",
    "            'from': email_data['from'],\n",
    "            'to': email_data['to'],\n",
    "            'date': email_data['date'],\n",
    "            'filename': filename\n",
    "        }\n",
    "        \n",
    "        # Combine subject and body for text processing\n",
    "        full_text = f\"Subject: {email_data['subject']}\\n\\n{email_data['body']}\"\n",
    "        email_texts[email_name] = full_text\n",
    "        \n",
    "        word_count = len(full_text.split())\n",
    "        print(f\"  Extracted {word_count} words from {filename}\")\n",
    "        print(f\"  Subject: {email_data['subject'][:50]}...\")\n",
    "    else:\n",
    "        print(f\"  Failed to parse {filename}\")\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(email_texts)} emails\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/raw_emails/First Merchants Bank Royal Oak_ Invitation to bid on First Merchants Bank.eml...\n",
      "  Extracted 569 words from data/raw_emails/First Merchants Bank Royal Oak_ Invitation to bid on First Merchants Bank.eml\n",
      "  Subject: First Merchants Bank Royal Oak: Invitation to bid ...\n",
      "\n",
      "Successfully processed 1 emails\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Email Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:12.893891Z",
     "start_time": "2025-09-24T15:22:12.869872Z"
    }
   },
   "source": [
    "# Create a summary DataFrame\n",
    "if email_metadata:\n",
    "    df = pd.DataFrame.from_dict(email_metadata, orient='index')\n",
    "    print(\"Email Summary:\")\n",
    "    print(df[['subject', 'from', 'date']].to_string())\n",
    "    \n",
    "    # Show word counts\n",
    "    print(\"\\nWord Counts:\")\n",
    "    for email_name, text in email_texts.items():\n",
    "        word_count = len(text.split())\n",
    "        print(f\"  {email_name}: {word_count} words\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Summary:\n",
      "                                                                           subject                                                                        from                             date\n",
      "Email-1  First Merchants Bank Royal Oak: Invitation to bid on First Merchants Bank  \"Brad Kecskemeti (PCI Industries, Inc)\" <notifications@update.procore.com>  Wed, 24 Sep 2025 15:03:18 +0000\n",
      "\n",
      "Word Counts:\n",
      "  Email-1: 569 words\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare for RAG Pipeline\n",
    "\n",
    "The extracted email texts are now ready for further processing in a RAG pipeline:\n",
    "- Text chunking\n",
    "- Vector embeddings\n",
    "- Vector store creation\n",
    "- Retrieval and generation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:31.113797Z",
     "start_time": "2025-09-24T15:22:31.109315Z"
    }
   },
   "source": [
    "# Example: Simple text chunking for RAG\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks for each email\n",
    "email_chunks = {}\n",
    "for email_name, text in email_texts.items():\n",
    "    chunks = chunk_text(text)\n",
    "    email_chunks[email_name] = chunks\n",
    "    print(f\"{email_name}: {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal chunks created: {sum(len(chunks) for chunks in email_chunks.values())}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email-1: 2 chunks\n",
      "\n",
      "Total chunks created: 2\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps for Full RAG Pipeline\n",
    "\n",
    "To complete the RAG pipeline, you would typically:\n",
    "\n",
    "1. **Generate Embeddings**: Use a model like Sentence-BERT to create vector embeddings for each chunk\n",
    "2. **Create Vector Store**: Store embeddings in a vector database (FAISS, Pinecone, etc.)\n",
    "3. **Implement Retrieval**: Create a function to find relevant chunks based on query similarity\n",
    "4. **Add Generation**: Use an LLM to generate responses based on retrieved chunks\n",
    "\n",
    "Example code structure:\n",
    "```python\n",
    "# Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Create vector store\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "def retrieve_chunks(query: str, k: int = 5):\n",
    "    query_embedding = model.encode([query])\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
